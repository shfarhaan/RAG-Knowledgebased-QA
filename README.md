# Agentic RAG System for Domain Knowledge QA

A sophisticated mini-RAG system with agentic capabilities for accurate, grounded domain knowledge question answering. The system combines document retrieval, self-reflection, and intelligent answer generation to minimize hallucinations.

## ğŸ¯ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agentic RAG System         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Tool: Retriever     â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ Critic: Evaluator   â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ Generator: LLM      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grounded       â”‚
â”‚  Answer with    â”‚
â”‚  Citations      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Features

### 1. **Document Processing**
- Intelligent document chunking with overlap
- Support for multiple formats (.txt, .md, .pdf)
- Metadata preservation for source tracking

### 2. **Embedding & Vector Storage**
- Azure OpenAI text-embedding-ada-002 embeddings
- FAISS vector database for efficient similarity search
- Persistent storage and loading of vectors

### 3. **Retrieval Logic**
- Context-aware document retrieval
- Similarity ranking and filtering
- Formatted context extraction

### 4. **Agentic Components**
- **Tool Use**: Document retriever as callable tool
- **Self-Reflection**: Critic evaluates document relevance and coverage
- **Answer Generation**: LLM generates grounded responses with citations
- **Iterative Refinement**: Multi-iteration retrieval with query refinement

### 5. **Streamlit UI**
- Intuitive chat interface
- Document processing pipeline
- System information dashboard
- Conversation history with metadata

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Azure OpenAI API credentials (API Key and Endpoint)

### Installation

1. **Clone/Create the project:**
```bash
cd "f:\Selise Assessment"
```

2. **Set up virtual environment and install dependencies:**

**Option A: Automatic Setup (Recommended) â­**
```bash
# Windows: One command does everything!
install.bat

# This will:
# 1. Create virtual environment (venv/)
# 2. Activate it automatically  
# 3. Upgrade pip
# 4. Install all dependencies
```

**Option B: Manual Setup**
```bash
# Step 1: Create virtual environment
python -m venv venv

# Step 2: Activate it
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# Windows Command Prompt:
venv\Scripts\activate.bat
# Mac/Linux:
source venv/bin/activate

# Step 3: You should see (venv) in your prompt
# Step 4: Install dependencies
pip install -r requirements.txt

# Step 5: Verify setup
python setup.py
```

> **ğŸ’¡ Important**: Always activate the virtual environment before running the application!
> See [VIRTUAL_ENV_GUIDE.md](VIRTUAL_ENV_GUIDE.md) for detailed instructions.

3. **Set up API credentials:**

Create a `.env` file in the project root:
```env
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource-name.cognitiveservices.azure.com
```

Or configure Streamlit secrets:
```bash
mkdir .streamlit
# Create .streamlit/secrets.toml with Azure OpenAI credentials
```

### Running the System

**Always activate your virtual environment first!**
```bash
# Windows PowerShell
.\venv\Scripts\Activate.ps1

# Windows Command Prompt
venv\Scripts\activate.bat

# Mac/Linux
source venv/bin/activate

# You should see (venv) in your prompt
```

**Then start the Streamlit application:**
```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`

> **Tip**: If you get "command not found" errors, make sure your virtual environment is activated (look for `(venv)` in your prompt).

## ğŸ“š Usage Guide

### Step 1: Process Documents

1. Navigate to the **"Process Documents"** tab
2. Configure chunking parameters (optional):
   - Chunk Size: 1000 (default)
   - Chunk Overlap: 200 (default)
3. Upload .txt or .md files containing your domain knowledge
4. Click **"Process Documents"** button

The system will:
- Load and parse documents
- Split into chunks with overlap
- Generate embeddings using Google's API
- Store in FAISS vector database

### Step 2: Ask Questions

1. Navigate to the **"Ask Questions"** tab
2. Enter your question in the chat input
3. The agent will:
   - Retrieve relevant documents
   - Evaluate document relevance (critic)
   - Generate a grounded answer with citations

### Step 3: Monitor Performance

- View **Confidence Score**: How confident is the system in this answer?
- View **Iterations**: How many retrieval attempts were made?
- Check **Metadata**: See which documents were used

## ğŸ—ï¸ Project Structure

```
f:\Selise Assessment/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ document_processor.py  # Document loading and chunking
â”‚   â”œâ”€â”€ embeddings.py          # Embedding generation and FAISS storage
â”‚   â”œâ”€â”€ retriever.py           # Retrieval logic
â”‚   â””â”€â”€ agent.py               # Agentic RAG with reflection
â”œâ”€â”€ documents/                 # Your domain documents
â”‚   â”œâ”€â”€ ml_fundamentals.txt
â”‚   â””â”€â”€ rag_guide.txt
â”œâ”€â”€ vector_store/              # FAISS index and metadata
â”œâ”€â”€ app.py                     # Streamlit UI
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # API keys (not in git)
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Configuration

Edit `src/config.py` to customize:

```python
# Model Configuration
AZURE_OPENAI_API_KEY = "your-key"              # Azure OpenAI API key
AZURE_OPENAI_ENDPOINT = "your-endpoint"         # Azure OpenAI endpoint
CHAT_DEPLOYMENT_NAME = "gpt-4o-mini"            # Chat model deployment
EMBEDDING_DEPLOYMENT_NAME = "text-embedding-ada-002"  # Embedding deployment
TEMPERATURE = 0.3                                # Response temperature

# Chunking
CHUNK_SIZE = 1000              # Size of each chunk
CHUNK_OVERLAP = 200            # Overlap between chunks

# Retrieval
TOP_K_DOCUMENTS = 5            # Number of documents to retrieve
SIMILARITY_THRESHOLD = 0.3     # Minimum relevance score

# Paths
DOCUMENTS_PATH = "documents"
VECTOR_STORE_PATH = "vector_store/faiss_index"
```

## ğŸ¤– Agentic RAG Workflow

### Agent Loop

1. **Initial Request**: User asks a question
2. **Retrieval Tool**: Agent calls retriever to get relevant documents
3. **Critic Evaluation**: Evaluates relevance and coverage of retrieved docs
4. **Reflection**: Checks if retrieval quality is sufficient
5. **Query Refinement** (if needed): Refine query and retry retrieval
6. **Answer Generation**: Generate final answer grounded in documents
7. **Response**: Return answer with confidence score and citations

### Key Algorithms

**Embedding Generation:**
- Uses Azure OpenAI's `text-embedding-ada-002` model
- Dense vector representations of text (1536 dimensions)

**Similarity Search:**
- FAISS L2 distance metric
- Top-K retrieval with ranking
- Distance to similarity conversion: `similarity = 1 / (1 + distance)`

**Critic Evaluation:**
- Evaluates relevance, coverage, and confidence
- Recommends re-retrieval if quality is low
- Structured JSON output

## ğŸ¯ Minimizing Hallucinations

The system minimizes hallucinations through:

1. **Grounding**: All answers based on retrieved documents
2. **Explicit Acknowledgment**: States when information is unavailable
3. **Source Citations**: References which documents were used
4. **Confidence Scoring**: Shows how confident the system is
5. **Threshold Filtering**: Ignores low-relevance documents
6. **Critic Evaluation**: Validates retrieved document quality
7. **Iterative Refinement**: Multiple retrieval attempts if needed

## ğŸ“Š Example Queries

With the provided sample documents, try:

**On ML Fundamentals:**
- "What are the different types of machine learning?"
- "How do we prevent overfitting in machine learning?"
- "Explain the difference between precision and recall"

**On RAG Systems:**
- "What are the advantages of RAG systems?"
- "How do vector databases improve retrieval?"
- "What is the difference between FAISS and Pinecone?"

**Cross-domain:**
- "Compare machine learning and RAG systems"
- "How do embeddings work in RAG?"

## ğŸ” Troubleshooting

### Issue: Import warnings in VS Code (e.g., "Import 'streamlit' could not be resolved")
- **Cause**: Packages not yet installed
- **Solution**: 
  ```bash
  # Run the installer
  install.bat
  
  # Or install manually
  pip install -r requirements.txt
  
  # Or verify setup
  python setup.py
  ```
- These warnings will disappear after installation

### Issue: "Azure OpenAI credentials not found"
- **Solution**: Set your API key and endpoint in `.env` file or Streamlit secrets
- Check both `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT` are properly set

### Issue: "No relevant documents found"
- **Solution**: Process documents first in the "Process Documents" tab

### Issue: "Vector store files not found"
- **Solution**: Process documents to create the vector store

### Issue: Slow embeddings generation
- **Solution**: This is expected for large document sets. The API calls can take time.

### Issue: Out of memory with FAISS
- **Solution**: Reduce chunk size or use vector database service (Pinecone, Qdrant)

## ğŸš€ Performance Optimization

### For Large Document Sets

1. **Use external vector database:**
```python
# Replace FAISS with Qdrant:
from qdrant_client import QdrantClient
vector_store = QdrantClient(":memory:")
```

2. **Implement batch processing:**
```python
embeddings = embedding_manager.embed_batch(texts, batch_size=50)
```

3. **Use caching:**
```python
@st.cache_resource
def initialize_rag_system():
    ...
```

## ğŸ“ˆ Evaluation Metrics

The system tracks:
- **Relevance Score**: 0-100% relevance of retrieved docs
- **Coverage Score**: 0-100% how well docs cover the query
- **Confidence Score**: 0-100% confidence in the answer
- **Iterations**: Number of retrieval attempts needed

## ğŸ” Security Considerations

- Store API keys in `.env` or Streamlit secrets, never in code
- Validate and sanitize user inputs
- Rate-limit API calls for production
- Implement authentication for sensitive deployments

## ğŸ“š Further Reading

### RAG Systems
- [The Power of Semantic Search](https://arxiv.org/abs/2004.04906)
- [In-Context Retrieval-Augmented Language Models](https://arxiv.org/abs/2302.07402)
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.07016)

### Vector Databases
- [FAISS: A Library for Efficient Similarity Search](https://arxiv.org/abs/1702.08734)
- [Qdrant Vector Search Engine Documentation](https://qdrant.tech/documentation/)

### Large Language Models
- [Azure OpenAI Service Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)
- [OpenAI API Documentation](https://platform.openai.com/docs/)
- [LangChain Documentation](https://python.langchain.com/)

## ğŸ¤ Contributing

Suggestions for improvements:
1. Add support for more document formats
2. Implement ensemble retrieval
3. Add support for multiple embedding models
4. Implement caching for embeddings
5. Add metrics dashboard
6. Support for images in documents

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™‹ Support

For issues or questions:
1. Check the Troubleshooting section
2. Review logs in the terminal
3. Ensure API key is configured correctly
4. Verify documents are processed first

## ğŸ“ Notes

- The system uses synchronous API calls. For production, consider async implementation.
- FAISS is suitable for <100M vectors. For larger scales, use professional services.
- Embedding generation has API rate limits. Monitor your usage.
- The critic evaluation requires additional API calls. Budget accordingly.

---

**Built with**: Azure OpenAI â€¢ FAISS â€¢ Streamlit â€¢ Python

**Last Updated**: February 2026
